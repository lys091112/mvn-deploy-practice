FROM lys091112/ubuntu:0.1

ENV JAVA_ADDRESS http://10.252.25.90:8125/files/jdk-8u181-linux-x64.tar.gz
ENV SCALA_ADDRESS http://10.252.25.90:8125/files/scala-2.11.8.tgz
ENV HADOOP_ADDRESS http://10.252.25.90:8125/files/hadoop-3.0.3.tar.gz
ENV SPARK_ADDRESS http://10.252.25.90:8125/files/spark-2.3.1-bin-hadoop2.7.tgz


ENV JAVA_HOME /usr/crescent/jdk1.8.0_181
ENV SCALA_HOME /usr/crescent/scala-2.11.8
ENV HADOOP_HOME /usr/crescent/hadoop-3.0.3
ENV SPARK_HOME /usr/crescent/spark-2.3.1-bin-hadoop2.7

# Add build payloads
ADD resources/bin /bin
ADD resources/ssh /root/.ssh
ADD resources/hadoop /tmp/conf
ADD resources/spark /tmp/spark

RUN \

    chmod 755 /bin/*.sh \

    && chown root:root -R ./root \
    && chmod 600 /root/.ssh/authorized_keys \
    && echo "PermitRootLogin yes" >> /etc/ssh/sshd_config \

    && mkdir -p /usr/crescent \

    # Install Java
    && curl -fSL $JAVA_ADDRESS -o /tmp/java.tar.gz \
    && tar -zxf /tmp/java.tar.gz -C /usr/crescent/ \
    && echo "export JAVA_HOME="$JAVA_HOME >> /etc/profile \
    && echo "export JRE_HOME=\${JAVA_HOME}/jre" >> /etc/profile \
    && echo "export CLASSPATH=.:\${JAVA_HOME}/lib:\${JRE_HOME}/lib" >> /etc/profile \
    && echo "export PATH=\${JAVA_HOME}/bin:\$PATH" >> /etc/profile \
    && rm -rf /tmp/java.tar.gz \

    # Install Scala
    && curl -fSL $SCALA_ADDRESS -o /tmp/scala.tgz \
    && tar -zxf /tmp/scala.tgz -C /usr/crescent/ \
    && echo "export SCALA_HOME="$SCALA_HOME >> /etc/profile \
    && echo "export PATH=\${SCALA_HOME}/bin:\$PATH" >> /etc/profile \
    && rm -rf /tmp/scala.tgz \


    ## Install Hadoop
    && curl -fSL $HADOOP_ADDRESS -o /tmp/hadoop.tar.gz \
    && tar -zxf /tmp/hadoop.tar.gz -C /usr/crescent/ \
    && echo "export HADOOP_HOME="$HADOOP_HOME >> /etc/profile \
    && echo "export PATH=\${HADOOP_HOME}/bin:\${HADOOP_HOME}/sbin:\$PATH" >> /etc/profile \
    && echo "export HADOOP_PREFIX="$HADOOP_HOME >> /usr/crescent/hadoop-3.0.3/etc/hadoop/hadoop-env.sh \
    && echo "export JAVA_HOME="$JAVA_HOME >> /usr/crescent/hadoop-3.0.3/etc/hadoop/hadoop-env.sh \
    && echo "export JAVA_HOME="$JAVA_HOME >> /usr/crescent/hadoop-3.0.3/etc/hadoop/yarm-env.sh \
    && yes | cp -r /tmp/conf/* /usr/crescent/hadoop-3.0.3/etc/hadoop/ \
    && rm -rf /tmp/hadoop.tar.gz \

    ## Install Spark
    && curl -fSL $SPARK_ADDRESS -o /tmp/spark.tar.gz \
    && tar -zxf /tmp/spark.tar.gz -C /usr/crescent/ \
    && echo "export SPARK_HOME="$SPARK_HOME >> /etc/profile \
    && echo "export PATH=\${SPARK_HOME}/bin:\${SPARK_HOME}/sbin:\$PATH" >> /etc/profile \
    && mv /usr/crescent/spark-2.3.1-bin-hadoop2.7/conf/spark-env.sh.template /usr/crescent/spark-2.3.1-bin-hadoop2.7/conf/spark-env.sh \
    && echo "export JAVA_HOME="$JAVA_HOME >> /usr/crescent/spark-2.3.1-bin-hadoop2.7/conf/spark-env.sh \
    && echo "export SCALA_HOME="$SCALA_HOME >> /usr/crescent/spark-2.3.1-bin-hadoop2.7/conf/spark-env.sh \
    && echo "export SPARK_WORKER_MEMORY=1g" >> /usr/crescent/spark-2.3.1-bin-hadoop2.7/conf/spark-env.sh \
    && echo "export HADOOP_CONF_DIR=/usr/crescent/hadoop-3.0.3/etc/hadoop" >> /usr/crescent/spark-2.3.1-bin-hadoop2.7/conf/spark-env.sh \
    && echo "export SPARK_MASTER_HOST=master" >> /usr/crescent/spark-2.3.1-bin-hadoop2.7/conf/spark-env.sh \
    && yes | cp /tmp/spark/* ${SPARK_HOME}/conf/ \
    && rm -rf /tmp/spark.tar.gz
    
# Add JAVA_HOME into system environment
ENV CLASSPATH .:${JAVA_HOME}/lib:${JRE_HOME}/lib
ENV PATH $PATH:$JAVA_HOME/bin:$SCALA_HOME/lib:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin

ENV HDFS_DATANODE_USER=root
ENV HADOOP_SECURE_DN_USER=hdfs
ENV HDFS_NAMENODE_USER=root
ENV HDFS_SECONDARYNAMENODE_USER=root

ENV YARN_RESOURCEMANAGER_USER=root
ENV HADOOP_SECURE_DN_USER=yarn
ENV YARN_NODEMANAGER_USER=root

ENTRYPOINT ["/bin/bash","/bin/startup.sh"]
